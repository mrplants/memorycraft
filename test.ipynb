{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9443424-d415-41cf-98e5-5e5ef041e3a0",
   "metadata": {},
   "source": [
    "# Test Notebook\n",
    "## Play with functions and new APIs before adding them to the test lineup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "914d9437-9a24-4f1e-8a6b-f11f76271e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_centroids(c):\n",
    "    for index, image in enumerate(c):\n",
    "        image = image.numpy().astype(float).reshape(64,64,3)\n",
    "        image /= 255\n",
    "        print(f\"Centroid {index}:\")\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        \n",
    "def print_centroid(c):\n",
    "    image = c\n",
    "    image = image.numpy().astype(float).reshape(64,64,3)\n",
    "    image /= 255\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d25b1-520c-4fb4-9921-a47c59003764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import util\n",
    "import tensorflow as tf\n",
    "\n",
    "def tf_kmeans(dataset: tf.data.Dataset,\n",
    "              k:int,\n",
    "              batch_size:int,\n",
    "              max_iterations:int=300,\n",
    "              min_inertia:float=0.0001,\n",
    "              init_centroids:tf.Tensor=None,\n",
    "              mean:float=0,\n",
    "              std:float=1) -> tf.Tensor:\n",
    "    \"\"\" Fits the data using kmeans clustering with Tensorflow.\n",
    "    \n",
    "    Args:\n",
    "      data: Data iterator compatible with Tensorflow (e.g., tf.data.Dataset)\n",
    "        Produces tf.Tensors with shape (n_features,).\n",
    "      k: The number of clusters.\n",
    "      batch_size: The number of samples per training batch.\n",
    "      init_centroids: If not provided, the first k points will be chosen as the\n",
    "        starting centroids.\n",
    "      mean: used for normalizing the samples\n",
    "      std: used for normalizing the samples\n",
    "      \n",
    "    Returns:  The fitted centroids\n",
    "    \"\"\"\n",
    "    # Initialize centroids with the first few points\n",
    "    if init_centroids == None:\n",
    "        centroids = tf.cast(tf.stack([f for f in ds.take(k)]), tf.float32)\n",
    "        while centroids.shape[0] < k:\n",
    "            more = tf.cast(tf.stack([f for f in ds.take(k-centroids.shape[0])]), tf.float32)\n",
    "            centroids = tf.concat([centroids, more], axis=0)\n",
    "        centroids = tf.Variable((centroids - mean) / std)\n",
    "    else:\n",
    "        centroids = tf.Variable(init_centroids)\n",
    "    next_round_centroids = tf.Variable(centroids)\n",
    "    # Keep track of inertia to determine convergence\n",
    "    last_inertia = None\n",
    "    # Keep track of total iterations in case max_iterations has been provided\n",
    "    num_iterations = 0\n",
    "    while True:\n",
    "        # Keep track of counts of points assigned to each centroid\n",
    "        # (for calculating online average)\n",
    "        centroid_counts = tf.zeros(k, dtype=tf.float32)\n",
    "        inertia = 0\n",
    "        for batch in ds.batch(batch_size):\n",
    "            batch = tf.cast(batch, tf.float32)\n",
    "            batch = (batch - mean) / std\n",
    "            # Assign samples each to its closest centroid\n",
    "            diffs = centroids - tf.expand_dims(batch, axis=1)\n",
    "            # diffs axis 0 is the sample, axis 1 is the centroid\n",
    "            norms = tf.norm(diffs, axis=2)\n",
    "            # Assign centroids to each sample and save the total inertia\n",
    "            assigned_centroids = tf.math.argmin(norms, axis=1)\n",
    "            batch_inertia = tf.gather_nd(norms, tf.reshape(assigned_centroids, (-1,1)), batch_dims=1)\n",
    "            inertia += tf.reduce_sum(batch_inertia).numpy().item()\n",
    "            # Update counts for each centroid\n",
    "            batch_centroid_counts = tf.math.bincount(assigned_centroids, minlength=k, axis=0, dtype=tf.float32)\n",
    "            new_centroid_counts = centroid_counts + batch_centroid_counts\n",
    "            # Update the next round centroids as a running mean\n",
    "            # Start by calculating the mean of this batch's assigned points\n",
    "            c_update_mean = []\n",
    "            for c_index in range(k):\n",
    "                c_assigned_indexes = tf.where(assigned_centroids == c_index)\n",
    "                if c_assigned_indexes.shape[0] != 0:\n",
    "                    c_update_mean.append(tf.reduce_mean(tf.gather_nd(batch, c_assigned_indexes), axis=0))\n",
    "                else:\n",
    "                    c_update_mean.append(centroids[c_index])\n",
    "            c_update_mean = tf.stack(c_update_mean)\n",
    "            update_fractions = tf.math.divide_no_nan(centroid_counts, new_centroid_counts)\n",
    "            next_round_centroids.assign(next_round_centroids * update_fractions[:,tf.newaxis] + c_update_mean * (1-update_fractions)[:,tf.newaxis])\n",
    "            # Update running counts for each centroid\n",
    "            centroid_counts = new_centroid_counts\n",
    "        # Determine convergence criteria.  Break if converged.\n",
    "        inertia /= tf.reduce_sum(centroid_counts).numpy().item()\n",
    "        if last_inertia != None and tf.math.abs(last_inertia - inertia) < min_inertia:\n",
    "            break\n",
    "        last_inertia = inertia\n",
    "        num_iterations += 1\n",
    "        if num_iterations >= max_iterations:\n",
    "            break\n",
    "        print(inertia)\n",
    "        # Update centroids from next_round_centroids\n",
    "        centroids.assign(next_round_centroids)\n",
    "    print(centroid_counts.numpy().astype(int).tolist())\n",
    "    return tf.identity(centroids) * std + mean\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "ds = util.build_frames_dataset().map(lambda x: tf.reshape(x, [-1])).shuffle(50000).prefetch(10000)\n",
    "# ds = tf.data.Dataset.from_tensors(list(ds.take(5000))).unbatch()\n",
    "centroids = tf_kmeans(ds, k=20, batch_size=5000, mean=128, std=255)\n",
    "print_centroids(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108121b7-cc1a-496d-bc11-fcb6cf3056e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
